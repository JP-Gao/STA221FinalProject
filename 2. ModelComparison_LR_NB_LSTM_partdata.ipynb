{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The notebook is run on 30k data. However, the results in the report are generated by 100k data which are run on alan server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from random import sample\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from gensim.models import KeyedVectors\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemmed_words(doc): return (stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some configuration\n",
    "B = 100000\n",
    "seed = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "train = pd.read_csv('new_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 12)\n"
     ]
    }
   ],
   "source": [
    "train_sub = train.sample(n = B, random_state = seed)\n",
    "print(train_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF-IDF matrix\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "tfidf_vectorizer=TfidfVectorizer(analyzer=stemmed_words)\n",
    "\n",
    "Tfidf_train_vector=tfidf_vectorizer.fit_transform(train_sub.loc[:,\"question_text\"])\n",
    "\n",
    "Tfidf_train_df = pd.DataFrame(Tfidf_train_vector.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "Tfidf_train = Tfidf_train_df.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the matrix with the newly constructed features in EDA part.\n",
    "new_feature_train = train_sub.loc[:,[\"num_words\",\"num_unique_words\",\"num_punctuations\",\"num_words_upper\",\"num_words_title\",\"mean_word_len\"]].to_numpy()\n",
    "\n",
    "train_X = np.concatenate((Tfidf_train, new_feature_train),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21000, 20167), (9000, 20167), (21000,), (9000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train validation and test split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(train_X, train_sub['target'], test_size=0.3, random_state=0)\n",
    "X_tr.shape, X_val.shape, y_tr.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([0.1,1,10,50,100])\n",
    "C_score = np.zeros(len(C))\n",
    "C_f1score = np.zeros(len(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,i in zip(C,np.arange(len(C))):\n",
    "    lreg = LogisticRegression(solver = 'liblinear', penalty='l1', C=c).fit(X_tr, y_tr)\n",
    "    C_score[i] = lreg.score(X_val, y_val)\n",
    "    C_f1score[i] = f1_score(lreg.predict(X_val),y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score for Logistic Regression with C = [0.1, 1, 10, 50, 100], is [0.14710485 0.41826923 0.44989775 0.41722488 0.40607211]\n"
     ]
    }
   ],
   "source": [
    "print(\"The F1 score for Logistic Regression with C = [0.1, 1, 10, 50, 100], is {}\".format(C_f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '622ae9e6e2f4c8e2fb71'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0a4c3306180e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mf1score_GNB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The F1 score for Gaussian Naive Bayes is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1score_MNB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m--> 208\u001b[0;31m                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '622ae9e6e2f4c8e2fb71'"
     ]
    }
   ],
   "source": [
    "mnb = GaussianNB().fit(X_tr, y_tr)\n",
    "f1score_GNB = f1_score(gnb.predict(X_val), y_val)\n",
    "print(\"The F1 score for Gaussian Naive Bayes is {}\".format(f1score_MNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpretrained BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21000, 12) (9000, 12) (21000,) (9000,)\n"
     ]
    }
   ],
   "source": [
    "## split to train and val\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(train_sub, train_sub['target'], test_size=0.3, random_state=0)\n",
    "print(X_tr.shape, X_val.shape, y_tr.shape, y_val.shape)\n",
    "\n",
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a question to use\n",
    "\n",
    "## fill up the missing values\n",
    "train_X = X_tr[\"question_text\"].fillna(\"_na_\").values\n",
    "val_X = X_val[\"question_text\"].fillna(\"_na_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "\n",
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "\n",
    "## Get the target values\n",
    "train_y = y_tr.values\n",
    "val_y = y_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 80)           109120    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 3,110,761\n",
      "Trainable params: 3,110,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jp/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples, validate on 9000 samples\n",
      "Epoch 1/4\n",
      "21000/21000 [==============================] - 36s 2ms/step - loss: 0.4504 - accuracy: 0.8964 - val_loss: 0.2362 - val_accuracy: 0.9366\n",
      "Epoch 2/4\n",
      "21000/21000 [==============================] - 36s 2ms/step - loss: 0.2643 - accuracy: 0.9350 - val_loss: 0.2347 - val_accuracy: 0.9366\n",
      "Epoch 3/4\n",
      "21000/21000 [==============================] - 37s 2ms/step - loss: 0.2514 - accuracy: 0.9350 - val_loss: 0.2296 - val_accuracy: 0.9366\n",
      "Epoch 4/4\n",
      "21000/21000 [==============================] - 38s 2ms/step - loss: 0.2428 - accuracy: 0.9350 - val_loss: 0.2071 - val_accuracy: 0.9366\n",
      "9000/9000 [==============================] - 5s 546us/step\n",
      "F1 score at threshold 0.1 is 0.3698296836982968\n",
      "F1 score at threshold 0.11 is 0.2868965517241379\n",
      "F1 score at threshold 0.12 is 0.20512820512820512\n",
      "F1 score at threshold 0.13 is 0.13759999999999997\n",
      "F1 score at threshold 0.14 is 0.11111111111111112\n",
      "F1 score at threshold 0.15 is 0.07679465776293824\n",
      "F1 score at threshold 0.16 is 0.04753820033955857\n",
      "F1 score at threshold 0.17 is 0.027538726333907054\n",
      "F1 score at threshold 0.18 is 0.006968641114982579\n",
      "F1 score at threshold 0.19 is 0.006980802792321117\n",
      "F1 score at threshold 0.2 is 0.003496503496503496\n",
      "F1 score at threshold 0.21 is 0.003496503496503496\n",
      "F1 score at threshold 0.22 is 0.0\n",
      "F1 score at threshold 0.23 is 0.0\n",
      "F1 score at threshold 0.24 is 0.0\n",
      "F1 score at threshold 0.25 is 0.0\n",
      "F1 score at threshold 0.26 is 0.0\n",
      "F1 score at threshold 0.27 is 0.0\n",
      "F1 score at threshold 0.28 is 0.0\n",
      "F1 score at threshold 0.29 is 0.0\n",
      "F1 score at threshold 0.3 is 0.0\n",
      "F1 score at threshold 0.31 is 0.0\n",
      "F1 score at threshold 0.32 is 0.0\n",
      "F1 score at threshold 0.33 is 0.0\n",
      "F1 score at threshold 0.34 is 0.0\n",
      "F1 score at threshold 0.35 is 0.0\n",
      "F1 score at threshold 0.36 is 0.0\n",
      "F1 score at threshold 0.37 is 0.0\n",
      "F1 score at threshold 0.38 is 0.0\n",
      "F1 score at threshold 0.39 is 0.0\n",
      "F1 score at threshold 0.4 is 0.0\n",
      "F1 score at threshold 0.41 is 0.0\n",
      "F1 score at threshold 0.42 is 0.0\n",
      "F1 score at threshold 0.43 is 0.0\n",
      "F1 score at threshold 0.44 is 0.0\n",
      "F1 score at threshold 0.45 is 0.0\n",
      "F1 score at threshold 0.46 is 0.0\n",
      "F1 score at threshold 0.47 is 0.0\n",
      "F1 score at threshold 0.48 is 0.0\n",
      "F1 score at threshold 0.49 is 0.0\n",
      "F1 score at threshold 0.5 is 0.0\n",
      "F1 score at threshold 0.51 is 0.0\n",
      "F1 score at threshold 0.52 is 0.0\n",
      "F1 score at threshold 0.53 is 0.0\n",
      "F1 score at threshold 0.54 is 0.0\n",
      "F1 score at threshold 0.55 is 0.0\n",
      "F1 score at threshold 0.56 is 0.0\n",
      "F1 score at threshold 0.57 is 0.0\n",
      "F1 score at threshold 0.58 is 0.0\n",
      "F1 score at threshold 0.59 is 0.0\n",
      "F1 score at threshold 0.6 is 0.0\n"
     ]
    }
   ],
   "source": [
    "# Jue's newly constructed NN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(40,dropout=0.2, recurrent_dropout=0.3,return_sequences=True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(20,activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(train_X, train_y, batch_size=1024, epochs=4, validation_data=(val_X, val_y))\n",
    "\n",
    "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.601, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_noemb_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained BiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 100, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 80)           109120    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 3,110,761\n",
      "Trainable params: 3,110,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 21000 samples, validate on 9000 samples\n",
      "Epoch 1/4\n",
      "21000/21000 [==============================] - 41s 2ms/step - loss: 0.3485 - accuracy: 0.9255 - val_loss: 0.2514 - val_accuracy: 0.9366\n",
      "Epoch 2/4\n",
      "21000/21000 [==============================] - 38s 2ms/step - loss: 0.2554 - accuracy: 0.9350 - val_loss: 0.2190 - val_accuracy: 0.9366\n",
      "Epoch 3/4\n",
      "21000/21000 [==============================] - 38s 2ms/step - loss: 0.2250 - accuracy: 0.9350 - val_loss: 0.1850 - val_accuracy: 0.9366\n",
      "Epoch 4/4\n",
      "21000/21000 [==============================] - 38s 2ms/step - loss: 0.1827 - accuracy: 0.9352 - val_loss: 0.1493 - val_accuracy: 0.9369\n",
      "9000/9000 [==============================] - 6s 627us/step\n",
      "For Word2Vec, F1 score at threshold 0.1 is 0.49418604651162784\n",
      "For Word2Vec, F1 score at threshold 0.11 is 0.5082467929138669\n",
      "For Word2Vec, F1 score at threshold 0.12 is 0.5136681500317865\n",
      "For Word2Vec, F1 score at threshold 0.13 is 0.521395655036208\n",
      "For Word2Vec, F1 score at threshold 0.14 is 0.5257452574525746\n",
      "For Word2Vec, F1 score at threshold 0.15 is 0.5266106442577031\n",
      "For Word2Vec, F1 score at threshold 0.16 is 0.5263157894736842\n",
      "For Word2Vec, F1 score at threshold 0.17 is 0.5285820341499629\n",
      "For Word2Vec, F1 score at threshold 0.18 is 0.5345622119815668\n",
      "For Word2Vec, F1 score at threshold 0.19 is 0.5451664025356577\n",
      "For Word2Vec, F1 score at threshold 0.2 is 0.5441295546558704\n",
      "For Word2Vec, F1 score at threshold 0.21 is 0.5345545378850957\n",
      "For Word2Vec, F1 score at threshold 0.22 is 0.5338983050847458\n",
      "For Word2Vec, F1 score at threshold 0.23 is 0.5343777197563099\n",
      "For Word2Vec, F1 score at threshold 0.24 is 0.5284697508896796\n",
      "For Word2Vec, F1 score at threshold 0.25 is 0.5214220601640838\n",
      "For Word2Vec, F1 score at threshold 0.26 is 0.5143918291550604\n",
      "For Word2Vec, F1 score at threshold 0.27 is 0.5113207547169811\n",
      "For Word2Vec, F1 score at threshold 0.28 is 0.513902205177373\n",
      "For Word2Vec, F1 score at threshold 0.29 is 0.5088062622309197\n",
      "For Word2Vec, F1 score at threshold 0.3 is 0.5044865403788634\n",
      "For Word2Vec, F1 score at threshold 0.31 is 0.4933469805527123\n",
      "For Word2Vec, F1 score at threshold 0.32 is 0.492179353493222\n",
      "For Word2Vec, F1 score at threshold 0.33 is 0.49096705632306054\n",
      "For Word2Vec, F1 score at threshold 0.34 is 0.48531011969532106\n",
      "For Word2Vec, F1 score at threshold 0.35 is 0.4749163879598662\n",
      "For Word2Vec, F1 score at threshold 0.36 is 0.4610091743119266\n",
      "For Word2Vec, F1 score at threshold 0.37 is 0.4380952380952381\n",
      "For Word2Vec, F1 score at threshold 0.38 is 0.41277641277641275\n",
      "For Word2Vec, F1 score at threshold 0.39 is 0.3876765083440309\n",
      "For Word2Vec, F1 score at threshold 0.4 is 0.35278514588859416\n",
      "For Word2Vec, F1 score at threshold 0.41 is 0.3173734610123119\n",
      "For Word2Vec, F1 score at threshold 0.42 is 0.29055007052186177\n",
      "For Word2Vec, F1 score at threshold 0.43 is 0.2311111111111111\n",
      "For Word2Vec, F1 score at threshold 0.44 is 0.18098159509202452\n",
      "For Word2Vec, F1 score at threshold 0.45 is 0.14218009478672985\n",
      "For Word2Vec, F1 score at threshold 0.46 is 0.11092985318107669\n",
      "For Word2Vec, F1 score at threshold 0.47 is 0.07058823529411765\n",
      "For Word2Vec, F1 score at threshold 0.48 is 0.03773584905660377\n",
      "For Word2Vec, F1 score at threshold 0.49 is 0.01733102253032929\n",
      "For Word2Vec, F1 score at threshold 0.5 is 0.010452961672473867\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = './embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary= True)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    if word in embeddings_index:\n",
    "        embedding_vector = embeddings_index.get_vector(word)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "\n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(40,dropout=0.2, recurrent_dropout=0.3, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(train_X, train_y, batch_size=1024, epochs=4, validation_data=(val_X, val_y))\n",
    "pred_word2vec_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"For Word2Vec, F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_word2vec_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jp/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 100, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 100, 80)           109120    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 3,110,761\n",
      "Trainable params: 3,110,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 21000 samples, validate on 9000 samples\n",
      "Epoch 1/4\n",
      "21000/21000 [==============================] - 39s 2ms/step - loss: 0.3764 - accuracy: 0.8640 - val_loss: 0.2365 - val_accuracy: 0.9366\n",
      "Epoch 2/4\n",
      "21000/21000 [==============================] - 32s 2ms/step - loss: 0.2608 - accuracy: 0.9348 - val_loss: 0.2197 - val_accuracy: 0.9366\n",
      "Epoch 3/4\n",
      "21000/21000 [==============================] - 33s 2ms/step - loss: 0.2383 - accuracy: 0.9349 - val_loss: 0.1928 - val_accuracy: 0.9366\n",
      "Epoch 4/4\n",
      "21000/21000 [==============================] - 40s 2ms/step - loss: 0.1982 - accuracy: 0.9350 - val_loss: 0.1557 - val_accuracy: 0.9366\n",
      "9000/9000 [==============================] - 6s 642us/step\n",
      "For Glove, F1 score at threshold 0.1 is 0.4954648526077098\n",
      "For Glove, F1 score at threshold 0.11 is 0.510204081632653\n",
      "For Glove, F1 score at threshold 0.12 is 0.5151898734177216\n",
      "For Glove, F1 score at threshold 0.13 is 0.530232558139535\n",
      "For Glove, F1 score at threshold 0.14 is 0.5454545454545454\n",
      "For Glove, F1 score at threshold 0.15 is 0.5471836137527432\n",
      "For Glove, F1 score at threshold 0.16 is 0.5518796992481204\n",
      "For Glove, F1 score at threshold 0.17 is 0.5413416536661467\n",
      "For Glove, F1 score at threshold 0.18 is 0.539967373572594\n",
      "For Glove, F1 score at threshold 0.19 is 0.5335598980458793\n",
      "For Glove, F1 score at threshold 0.2 is 0.5365853658536586\n",
      "For Glove, F1 score at threshold 0.21 is 0.5323741007194245\n",
      "For Glove, F1 score at threshold 0.22 is 0.5273909006499535\n",
      "For Glove, F1 score at threshold 0.23 is 0.5148895292987512\n",
      "For Glove, F1 score at threshold 0.24 is 0.5128712871287128\n",
      "For Glove, F1 score at threshold 0.25 is 0.49847405900305186\n",
      "For Glove, F1 score at threshold 0.26 is 0.4932079414838036\n",
      "For Glove, F1 score at threshold 0.27 is 0.4593716143011917\n",
      "For Glove, F1 score at threshold 0.28 is 0.45261984392419174\n",
      "For Glove, F1 score at threshold 0.29 is 0.4416475972540046\n",
      "For Glove, F1 score at threshold 0.3 is 0.4170616113744076\n",
      "For Glove, F1 score at threshold 0.31 is 0.39506172839506176\n",
      "For Glove, F1 score at threshold 0.32 is 0.3615384615384616\n",
      "For Glove, F1 score at threshold 0.33 is 0.32171581769436997\n",
      "For Glove, F1 score at threshold 0.34 is 0.2785515320334262\n",
      "For Glove, F1 score at threshold 0.35 is 0.24566473988439308\n",
      "For Glove, F1 score at threshold 0.36 is 0.21460506706408344\n",
      "For Glove, F1 score at threshold 0.37 is 0.1697530864197531\n",
      "For Glove, F1 score at threshold 0.38 is 0.12258064516129032\n",
      "For Glove, F1 score at threshold 0.39 is 0.08333333333333333\n",
      "For Glove, F1 score at threshold 0.4 is 0.030874785591766724\n",
      "For Glove, F1 score at threshold 0.41 is 0.006980802792321117\n",
      "For Glove, F1 score at threshold 0.42 is 0.006980802792321117\n",
      "For Glove, F1 score at threshold 0.43 is 0.0\n",
      "For Glove, F1 score at threshold 0.44 is 0.0\n",
      "For Glove, F1 score at threshold 0.45 is 0.0\n",
      "For Glove, F1 score at threshold 0.46 is 0.0\n",
      "For Glove, F1 score at threshold 0.47 is 0.0\n",
      "For Glove, F1 score at threshold 0.48 is 0.0\n",
      "For Glove, F1 score at threshold 0.49 is 0.0\n",
      "For Glove, F1 score at threshold 0.5 is 0.0\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = './embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(40,dropout=0.2, recurrent_dropout=0.3, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(train_X, train_y, batch_size=1024, epochs=4, validation_data=(val_X, val_y))\n",
    "pred_glove_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"For Glove, F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_glove_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples, validate on 9000 samples\n",
      "Epoch 1/4\n",
      "21000/21000 [==============================] - 37s 2ms/step - loss: 0.3519 - accuracy: 0.9215 - val_loss: 0.2502 - val_accuracy: 0.9366\n",
      "Epoch 2/4\n",
      "21000/21000 [==============================] - 38s 2ms/step - loss: 0.2524 - accuracy: 0.9350 - val_loss: 0.2275 - val_accuracy: 0.9366\n",
      "Epoch 3/4\n",
      "21000/21000 [==============================] - 40s 2ms/step - loss: 0.2377 - accuracy: 0.9350 - val_loss: 0.2102 - val_accuracy: 0.9366\n",
      "Epoch 4/4\n",
      "21000/21000 [==============================] - 38s 2ms/step - loss: 0.2093 - accuracy: 0.9350 - val_loss: 0.1725 - val_accuracy: 0.9366\n",
      "9000/9000 [==============================] - 6s 635us/step\n",
      "For fasttext, F1 score at threshold 0.1 is 0.449438202247191\n",
      "For fasttext, F1 score at threshold 0.11 is 0.4606879606879607\n",
      "For fasttext, F1 score at threshold 0.12 is 0.4726536124240379\n",
      "For fasttext, F1 score at threshold 0.13 is 0.4857768052516411\n",
      "For fasttext, F1 score at threshold 0.14 is 0.48857368006304175\n",
      "For fasttext, F1 score at threshold 0.15 is 0.49289891395154556\n",
      "For fasttext, F1 score at threshold 0.16 is 0.48586572438162545\n",
      "For fasttext, F1 score at threshold 0.17 is 0.47955390334572495\n",
      "For fasttext, F1 score at threshold 0.18 is 0.4683794466403162\n",
      "For fasttext, F1 score at threshold 0.19 is 0.44720496894409945\n",
      "For fasttext, F1 score at threshold 0.2 is 0.4312026002166847\n",
      "For fasttext, F1 score at threshold 0.21 is 0.4173318129988598\n",
      "For fasttext, F1 score at threshold 0.22 is 0.3933253873659118\n",
      "For fasttext, F1 score at threshold 0.23 is 0.3615960099750623\n",
      "For fasttext, F1 score at threshold 0.24 is 0.32765399737876805\n",
      "For fasttext, F1 score at threshold 0.25 is 0.2950819672131148\n",
      "For fasttext, F1 score at threshold 0.26 is 0.2549575070821529\n",
      "For fasttext, F1 score at threshold 0.27 is 0.23065693430656933\n",
      "For fasttext, F1 score at threshold 0.28 is 0.19696969696969696\n",
      "For fasttext, F1 score at threshold 0.29 is 0.16640502354788067\n",
      "For fasttext, F1 score at threshold 0.3 is 0.11783960720130932\n",
      "For fasttext, F1 score at threshold 0.31 is 0.060810810810810814\n",
      "For fasttext, F1 score at threshold 0.32 is 0.03424657534246575\n",
      "For fasttext, F1 score at threshold 0.33 is 0.010434782608695651\n",
      "For fasttext, F1 score at threshold 0.34 is 0.0034904013961605585\n",
      "For fasttext, F1 score at threshold 0.35 is 0.0\n",
      "For fasttext, F1 score at threshold 0.36 is 0.0\n",
      "For fasttext, F1 score at threshold 0.37 is 0.0\n",
      "For fasttext, F1 score at threshold 0.38 is 0.0\n",
      "For fasttext, F1 score at threshold 0.39 is 0.0\n",
      "For fasttext, F1 score at threshold 0.4 is 0.0\n",
      "For fasttext, F1 score at threshold 0.41 is 0.0\n",
      "For fasttext, F1 score at threshold 0.42 is 0.0\n",
      "For fasttext, F1 score at threshold 0.43 is 0.0\n",
      "For fasttext, F1 score at threshold 0.44 is 0.0\n",
      "For fasttext, F1 score at threshold 0.45 is 0.0\n",
      "For fasttext, F1 score at threshold 0.46 is 0.0\n",
      "For fasttext, F1 score at threshold 0.47 is 0.0\n",
      "For fasttext, F1 score at threshold 0.48 is 0.0\n",
      "For fasttext, F1 score at threshold 0.49 is 0.0\n",
      "For fasttext, F1 score at threshold 0.5 is 0.0\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = './embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(40,dropout=0.2, recurrent_dropout=0.3, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_y, batch_size=1024, epochs=4, validation_data=(val_X, val_y))\n",
    "pred_fasttext_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"For fasttext, F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_fasttext_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples, validate on 9000 samples\n",
      "Epoch 1/4\n",
      "21000/21000 [==============================] - 38s 2ms/step - loss: 0.3506 - accuracy: 0.8721 - val_loss: 0.2361 - val_accuracy: 0.9366\n",
      "Epoch 2/4\n",
      "21000/21000 [==============================] - 37s 2ms/step - loss: 0.2421 - accuracy: 0.9350 - val_loss: 0.2060 - val_accuracy: 0.9366\n",
      "Epoch 3/4\n",
      "21000/21000 [==============================] - 39s 2ms/step - loss: 0.2134 - accuracy: 0.9350 - val_loss: 0.1800 - val_accuracy: 0.9366\n",
      "Epoch 4/4\n",
      "21000/21000 [==============================] - 40s 2ms/step - loss: 0.1878 - accuracy: 0.9350 - val_loss: 0.1600 - val_accuracy: 0.9366\n",
      "9000/9000 [==============================] - 6s 626us/step\n",
      "For paragram, F1 score at threshold 0.1 is 0.4797092671108419\n",
      "For paragram, F1 score at threshold 0.11 is 0.4778987828315182\n",
      "For paragram, F1 score at threshold 0.12 is 0.4903397734843438\n",
      "For paragram, F1 score at threshold 0.13 is 0.49582172701949856\n",
      "For paragram, F1 score at threshold 0.14 is 0.4974765681326604\n",
      "For paragram, F1 score at threshold 0.15 is 0.494415487714073\n",
      "For paragram, F1 score at threshold 0.16 is 0.5042405551272167\n",
      "For paragram, F1 score at threshold 0.17 is 0.5094936708860759\n",
      "For paragram, F1 score at threshold 0.18 is 0.5094806265457542\n",
      "For paragram, F1 score at threshold 0.19 is 0.5161290322580646\n",
      "For paragram, F1 score at threshold 0.2 is 0.5132275132275133\n",
      "For paragram, F1 score at threshold 0.21 is 0.510948905109489\n",
      "For paragram, F1 score at threshold 0.22 is 0.5004686035613871\n",
      "For paragram, F1 score at threshold 0.23 is 0.4932821497120921\n",
      "For paragram, F1 score at threshold 0.24 is 0.4916420845624385\n",
      "For paragram, F1 score at threshold 0.25 is 0.488\n",
      "For paragram, F1 score at threshold 0.26 is 0.4781281790437436\n",
      "For paragram, F1 score at threshold 0.27 is 0.46492146596858636\n",
      "For paragram, F1 score at threshold 0.28 is 0.4577540106951872\n",
      "For paragram, F1 score at threshold 0.29 is 0.4473684210526316\n",
      "For paragram, F1 score at threshold 0.3 is 0.4337078651685393\n",
      "For paragram, F1 score at threshold 0.31 is 0.4060676779463243\n",
      "For paragram, F1 score at threshold 0.32 is 0.38989169675090257\n",
      "For paragram, F1 score at threshold 0.33 is 0.36454431960049943\n",
      "For paragram, F1 score at threshold 0.34 is 0.3350515463917526\n",
      "For paragram, F1 score at threshold 0.35 is 0.3021390374331551\n",
      "For paragram, F1 score at threshold 0.36 is 0.2888283378746594\n",
      "For paragram, F1 score at threshold 0.37 is 0.2538787023977433\n",
      "For paragram, F1 score at threshold 0.38 is 0.21700879765395892\n",
      "For paragram, F1 score at threshold 0.39 is 0.1924812030075188\n",
      "For paragram, F1 score at threshold 0.4 is 0.159375\n",
      "For paragram, F1 score at threshold 0.41 is 0.10372771474878444\n",
      "For paragram, F1 score at threshold 0.42 is 0.050590219224283306\n",
      "For paragram, F1 score at threshold 0.43 is 0.01391304347826087\n",
      "For paragram, F1 score at threshold 0.44 is 0.0\n",
      "For paragram, F1 score at threshold 0.45 is 0.0\n",
      "For paragram, F1 score at threshold 0.46 is 0.0\n",
      "For paragram, F1 score at threshold 0.47 is 0.0\n",
      "For paragram, F1 score at threshold 0.48 is 0.0\n",
      "For paragram, F1 score at threshold 0.49 is 0.0\n",
      "For paragram, F1 score at threshold 0.5 is 0.0\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = './embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(40,dropout=0.2, recurrent_dropout=0.3, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_y, batch_size=1024, epochs=4, validation_data=(val_X, val_y))\n",
    "pred_paragram_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"For paragram, F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_paragram_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
